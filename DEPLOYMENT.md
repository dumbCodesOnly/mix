# Deployment Guide\n\nThis guide covers deploying the AI Platform Backend to various platforms.\n\n## Prerequisites\n\nBefore deploying, ensure you have:\n\n1. A HuggingFace API key (get one at https://huggingface.co/settings/tokens)\n2. A supported deployment platform account\n3. Git repository with the code\n\n## Local Development\n\n### Using Docker Compose\n\n1. Create a `.env` file:\n```bash\ncp .env.example .env\n# Edit .env and add your HF_API_KEY\n```\n\n2. Start the services:\n```bash\ndocker-compose up\n```\n\n3. Access the API at `http://localhost:8000`\n\n### Using Python directly\n\n1. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Set environment variables:\n```bash\ncp .env.example .env\n# Edit .env and add your HF_API_KEY\nexport $(cat .env | xargs)\n```\n\n4. Run the server:\n```bash\npython -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\n```\n\n## Deployment Platforms\n\n### Render\n\nRender is the recommended platform for easy deployment.\n\n1. **Create a Render account** at https://render.com\n\n2. **Connect your Git repository**\n   - Click \"New +\" â†’ \"Web Service\"\n   - Connect your GitHub/GitLab account\n   - Select the repository\n\n3. **Configure the service**\n   - **Name**: `ai-platform-backend`\n   - **Environment**: `Python 3`\n   - **Build Command**: `pip install -r requirements.txt`\n   - **Start Command**: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`\n\n4. **Add environment variables**\n   - Click \"Environment\"\n   - Add `HF_API_KEY` with your HuggingFace API key\n   - Add other variables from `.env.example` as needed\n\n5. **Configure disk**\n   - Click \"Disks\"\n   - Add a disk for model caching (20GB recommended)\n   - Mount path: `/root/.cache`\n\n6. **Deploy**\n   - Click \"Create Web Service\"\n   - Render will automatically deploy on every push to main\n\n### Railway\n\n1. **Create a Railway account** at https://railway.app\n\n2. **Create a new project**\n   - Click \"New Project\"\n   - Select \"Deploy from GitHub\"\n   - Select your repository\n\n3. **Configure environment**\n   - Add variables from `.env.example`\n   - Set `HF_API_KEY` to your HuggingFace API key\n\n4. **Configure build**\n   - Set build command: `pip install -r requirements.txt`\n   - Set start command: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`\n\n5. **Deploy**\n   - Railway will automatically deploy\n\n### Heroku\n\n1. **Create a Heroku account** at https://heroku.com\n\n2. **Install Heroku CLI**\n```bash\ncurl https://cli-assets.heroku.com/install.sh | sh\n```\n\n3. **Create Procfile**\n```\nweb: uvicorn app.main:app --host 0.0.0.0 --port $PORT\n```\n\n4. **Deploy**\n```bash\nheroku login\nheroku create your-app-name\nheroku config:set HF_API_KEY=your_key_here\ngit push heroku main\n```\n\n### AWS Lambda\n\n1. **Install AWS CLI**\n```bash\npip install awscli\n```\n\n2. **Create Lambda function**\n```bash\naws lambda create-function \\\n  --function-name ai-platform-backend \\\n  --runtime python3.11 \\\n  --role arn:aws:iam::YOUR_ACCOUNT_ID:role/lambda-role \\\n  --handler app.main.app\n```\n\n3. **Deploy with API Gateway**\n   - Create an API Gateway\n   - Configure routes to Lambda function\n   - Set environment variables\n\n### Docker\n\n1. **Build Docker image**\n```bash\ndocker build -t ai-platform-backend:latest .\n```\n\n2. **Run container**\n```bash\ndocker run -p 8000:8000 \\\n  -e HF_API_KEY=your_key_here \\\n  ai-platform-backend:latest\n```\n\n3. **Push to Docker Hub**\n```bash\ndocker tag ai-platform-backend:latest your_username/ai-platform-backend:latest\ndocker push your_username/ai-platform-backend:latest\n```\n\n## Environment Variables\n\nKey environment variables for production:\n\n| Variable | Required | Default | Description |\n|----------|----------|---------|-------------|\n| `HF_API_KEY` | Yes | - | HuggingFace API key |\n| `HOST` | No | `0.0.0.0` | Server host |\n| `PORT` | No | `8000` | Server port |\n| `DEBUG` | No | `false` | Debug mode |\n| `ALLOWED_ORIGINS` | No | `http://localhost:3000` | CORS allowed origins |\n| `LOG_LEVEL` | No | `INFO` | Logging level |\n| `REQUEST_TIMEOUT` | No | `300` | Request timeout (seconds) |\n| `MAX_RETRIES` | No | `3` | Max retry attempts |\n\n## Monitoring\n\n### Health Checks\n\nAll platforms support health checks via:\n```\nGET /health\n```\n\nResponse:\n```json\n{\n  \"status\": \"healthy\",\n  \"timestamp\": \"2025-12-22T10:30:00Z\"\n}\n```\n\n### Logging\n\nLogs are output to stdout in JSON format by default. Configure via `LOG_FORMAT` environment variable.\n\n### Metrics\n\nMonitor these metrics:\n- Request latency\n- Error rate\n- Cache hit rate\n- Model inference time\n\n## Scaling\n\n### Vertical Scaling\n- Increase machine RAM and CPU\n- Useful for handling larger models\n\n### Horizontal Scaling\n- Deploy multiple instances behind a load balancer\n- Use container orchestration (Kubernetes)\n- Configure auto-scaling based on CPU/memory\n\n## Security\n\n### Best Practices\n\n1. **Never commit secrets**\n   - Use environment variables\n   - Use `.env.example` for templates\n\n2. **Use HTTPS**\n   - Enable SSL/TLS on your platform\n   - Redirect HTTP to HTTPS\n\n3. **Rate Limiting**\n   - Implement rate limiting in production\n   - Consider using a reverse proxy (nginx, Cloudflare)\n\n4. **CORS Configuration**\n   - Restrict `ALLOWED_ORIGINS` to your domains\n   - Don't use `*` in production\n\n5. **API Key Management**\n   - Rotate HuggingFace API keys regularly\n   - Use separate keys for different environments\n   - Monitor API key usage\n\n## Troubleshooting\n\n### Service won't start\n\n1. Check logs for errors\n2. Verify HF_API_KEY is set correctly\n3. Ensure all dependencies are installed\n4. Check port availability\n\n### High latency\n\n1. Check model inference time\n2. Verify network connectivity\n3. Monitor resource usage (CPU, memory)\n4. Consider upgrading to larger instance\n\n### Out of memory\n\n1. Increase instance RAM\n2. Reduce image size limits\n3. Clear cache regularly\n4. Use smaller models\n\n### API errors\n\n1. Check HuggingFace API status\n2. Verify API key permissions\n3. Check request parameters\n4. Review error logs\n\n## Maintenance\n\n### Regular Tasks\n\n1. **Monitor logs** - Check for errors and warnings\n2. **Update dependencies** - Keep packages up to date\n3. **Backup data** - If storing any persistent data\n4. **Review metrics** - Monitor performance trends\n5. **Test endpoints** - Verify all endpoints working\n\n### Updates\n\n1. Test updates in staging environment\n2. Use blue-green deployment for zero downtime\n3. Keep HuggingFace models updated\n4. Monitor breaking changes in dependencies\n\n## Cost Optimization\n\n1. **Choose appropriate instance size**\n   - Start small, scale as needed\n   - Monitor actual usage\n\n2. **Use caching**\n   - Cache model results\n   - Reduce API calls\n\n3. **Optimize images**\n   - Resize large images\n   - Compress output\n\n4. **Monitor costs**\n   - Set up billing alerts\n   - Review usage regularly\n\n## Support\n\nFor deployment issues:\n\n1. Check platform documentation\n2. Review application logs\n3. Verify environment configuration\n4. Test locally first\n5. Open an issue on GitHub\n
