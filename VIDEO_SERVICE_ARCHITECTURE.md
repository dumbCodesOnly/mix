# Video Service Architecture\n\n## Overview\n\nThe video service layer implements a **model-agnostic architecture** that seamlessly integrates with any HuggingFace video generation model. This design ensures flexibility, maintainability, and extensibility without requiring code changes when switching models.\n\n## Architecture Design\n\n### Core Principles\n\n1. **Model Agnostic**: No hardcoded model references\n2. **Configurable**: Models specified via environment variables or request parameters\n3. **Resilient**: Automatic retry and fallback support\n4. **Extensible**: Easy to add new video models\n5. **Performant**: Efficient caching and resource management\n\n### Service Layer Structure\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    Video Routers                         │\n│  ┌──────────────────┐  ┌──────────────────┐            │\n│  │ Text-to-Video    │  │ Image-to-Video   │            │\n│  │ Router           │  │ Router           │            │\n│  └──────────────────┘  └──────────────────┘            │\n└────────────┬──────────────────────────────┬─────────────┘\n             │                              │\n┌────────────▼──────────────────────────────▼─────────────┐\n│                    Video Service                         │\n│  ┌──────────────────────────────────────────────────┐  │\n│  │  generate_text_to_video()                        │  │\n│  │  generate_image_to_video()                       │  │\n│  │  _validate_input()                               │  │\n│  │  _process_video()                                │  │\n│  └──────────────────────────────────────────────────┘  │\n└────────────┬──────────────────────────────┬─────────────┘\n             │                              │\n┌────────────▼──────────────────────────────▼─────────────┐\n│           HuggingFace Client (hf_client.py)             │\n│  ┌──────────────────────────────────────────────────┐  │\n│  │  text_to_video(prompt, model, ...)              │  │\n│  │  image_to_video(image, model, ...)              │  │\n│  │  Retry Logic (Exponential Backoff)              │  │\n│  │  Model Fallback Support                         │  │\n│  │  Error Handling                                 │  │\n│  └──────────────────────────────────────────────────┘  │\n└────────────┬──────────────────────────────┬─────────────┘\n             │                              │\n┌────────────▼──────────────────────────────▼─────────────┐\n│              HuggingFace Inference API                   │\n│  ┌──────────────────────────────────────────────────┐  │\n│  │  Any HuggingFace Video Model                     │  │\n│  │  - Text-to-Video Models                         │  │\n│  │  - Image-to-Video Models                        │  │\n│  │  - Custom Models                                │  │\n│  └──────────────────────────────────────────────────┘  │\n└─────────────────────────────────────────────────────────┘\n```\n\n## Service Implementation\n\n### VideoService Class\n\n```python\nclass VideoService:\n    \"\"\"\n    Service for video generation using HuggingFace models.\n    \n    Model-agnostic design allows using any HuggingFace video model\n    without code changes.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the video service.\"\"\"\n        self.hf_client = get_hf_client()\n        self.logger = get_logger(__name__)\n    \n    async def generate_text_to_video(\n        self,\n        prompt: str,\n        model: Optional[str] = None,\n        negative_prompt: Optional[str] = None,\n        duration: int = 6,\n        fps: int = 8,\n        num_inference_steps: int = 50,\n    ) -> dict:\n        \"\"\"\n        Generate video from text prompt.\n        \n        Args:\n            prompt: Text description of video\n            model: HuggingFace model ID (uses default if None)\n            negative_prompt: What to avoid in video\n            duration: Video duration in seconds\n            fps: Frames per second\n            num_inference_steps: Quality/speed tradeoff\n            \n        Returns:\n            dict: Video data and metadata\n            \n        Raises:\n            ValidationError: If input is invalid\n            HuggingFaceAPIError: If API call fails\n            ProcessingError: If video processing fails\n        \"\"\"\n        # Use default model if not specified\n        model = model or Config.DEFAULT_TEXT_TO_VIDEO_MODEL\n        \n        # Validate input\n        self._validate_text_to_video_input(\n            prompt, duration, fps, num_inference_steps\n        )\n        \n        try:\n            self.logger.info(\n                f\"Generating text-to-video with model {model}\",\n                extra={\"prompt_length\": len(prompt), \"model\": model}\n            )\n            \n            # Call HuggingFace API\n            video_bytes = await self.hf_client.text_to_video(\n                prompt=prompt,\n                model=model,\n                negative_prompt=negative_prompt,\n                duration=duration,\n                fps=fps,\n                num_inference_steps=num_inference_steps,\n            )\n            \n            # Process and validate video\n            video_data = await self._process_video(\n                video_bytes, duration, fps\n            )\n            \n            self.logger.info(\n                \"Text-to-video generated successfully\",\n                extra={\"video_size\": len(video_bytes), \"model\": model}\n            )\n            \n            return video_data\n            \n        except HuggingFaceAPIError:\n            raise\n        except Exception as e:\n            self.logger.error(f\"Error generating text-to-video: {str(e)}\")\n            raise ProcessingError(\n                f\"Failed to generate video: {str(e)}\",\n                \"text_to_video\"\n            )\n    \n    async def generate_image_to_video(\n        self,\n        image_data: bytes,\n        model: Optional[str] = None,\n        prompt: Optional[str] = None,\n        duration: int = 6,\n        fps: int = 8,\n        num_inference_steps: int = 50,\n    ) -> dict:\n        \"\"\"\n        Generate video from image.\n        \n        Args:\n            image_data: Image bytes (PNG, JPG, WebP)\n            model: HuggingFace model ID (uses default if None)\n            prompt: Optional text prompt for video style\n            duration: Video duration in seconds\n            fps: Frames per second\n            num_inference_steps: Quality/speed tradeoff\n            \n        Returns:\n            dict: Video data and metadata\n            \n        Raises:\n            ValidationError: If input is invalid\n            FileSizeError: If image is too large\n            HuggingFaceAPIError: If API call fails\n            ProcessingError: If video processing fails\n        \"\"\"\n        # Use default model if not specified\n        model = model or Config.DEFAULT_IMAGE_TO_VIDEO_MODEL\n        \n        # Validate input\n        self._validate_image_to_video_input(\n            image_data, duration, fps, num_inference_steps\n        )\n        \n        try:\n            self.logger.info(\n                f\"Generating image-to-video with model {model}\",\n                extra={\"image_size\": len(image_data), \"model\": model}\n            )\n            \n            # Call HuggingFace API\n            video_bytes = await self.hf_client.image_to_video(\n                image=image_data,\n                model=model,\n                prompt=prompt,\n                duration=duration,\n                fps=fps,\n                num_inference_steps=num_inference_steps,\n            )\n            \n            # Process and validate video\n            video_data = await self._process_video(\n                video_bytes, duration, fps\n            )\n            \n            self.logger.info(\n                \"Image-to-video generated successfully\",\n                extra={\"video_size\": len(video_bytes), \"model\": model}\n            )\n            \n            return video_data\n            \n        except HuggingFaceAPIError:\n            raise\n        except Exception as e:\n            self.logger.error(f\"Error generating image-to-video: {str(e)}\")\n            raise ProcessingError(\n                f\"Failed to generate video: {str(e)}\",\n                \"image_to_video\"\n            )\n    \n    def _validate_text_to_video_input(\n        self,\n        prompt: str,\n        duration: int,\n        fps: int,\n        num_inference_steps: int,\n    ) -> None:\n        \"\"\"Validate text-to-video input parameters.\"\"\"\n        if not prompt or len(prompt) > 1000:\n            raise ValidationError(\n                \"Prompt must be 1-1000 characters\",\n                {\"prompt\": \"Invalid prompt length\"}\n            )\n        \n        if not 1 <= duration <= Config.MAX_VIDEO_DURATION:\n            raise ValidationError(\n                f\"Duration must be 1-{Config.MAX_VIDEO_DURATION} seconds\",\n                {\"duration\": f\"Must be between 1 and {Config.MAX_VIDEO_DURATION}\"}\n            )\n        \n        if not 1 <= fps <= 60:\n            raise ValidationError(\n                \"FPS must be 1-60\",\n                {\"fps\": \"Must be between 1 and 60\"}\n            )\n        \n        if not 1 <= num_inference_steps <= 100:\n            raise ValidationError(\n                \"Inference steps must be 1-100\",\n                {\"num_inference_steps\": \"Must be between 1 and 100\"}\n            )\n    \n    def _validate_image_to_video_input(\n        self,\n        image_data: bytes,\n        duration: int,\n        fps: int,\n        num_inference_steps: int,\n    ) -> None:\n        \"\"\"Validate image-to-video input parameters.\"\"\"\n        if len(image_data) > Config.MAX_IMAGE_SIZE:\n            raise FileSizeError(\n                len(image_data),\n                Config.MAX_IMAGE_SIZE,\n                \"image\"\n            )\n        \n        if not 1 <= duration <= Config.MAX_VIDEO_DURATION:\n            raise ValidationError(\n                f\"Duration must be 1-{Config.MAX_VIDEO_DURATION} seconds\",\n                {\"duration\": f\"Must be between 1 and {Config.MAX_VIDEO_DURATION}\"}\n            )\n        \n        if not 1 <= fps <= 60:\n            raise ValidationError(\n                \"FPS must be 1-60\",\n                {\"fps\": \"Must be between 1 and 60\"}\n            )\n        \n        if not 1 <= num_inference_steps <= 100:\n            raise ValidationError(\n                \"Inference steps must be 1-100\",\n                {\"num_inference_steps\": \"Must be between 1 and 100\"}\n            )\n    \n    async def _process_video(\n        self,\n        video_bytes: bytes,\n        duration: int,\n        fps: int,\n    ) -> dict:\n        \"\"\"\n        Process and validate generated video.\n        \n        Args:\n            video_bytes: Raw video data\n            duration: Expected duration\n            fps: Expected frames per second\n            \n        Returns:\n            dict: Processed video data and metadata\n        \"\"\"\n        # Validate video size\n        if len(video_bytes) > Config.MAX_VIDEO_FILE_SIZE:\n            raise FileSizeError(\n                len(video_bytes),\n                Config.MAX_VIDEO_FILE_SIZE,\n                \"video\"\n            )\n        \n        # Extract video metadata\n        resolution = await self._extract_resolution(video_bytes)\n        \n        return {\n            \"video_bytes\": video_bytes,\n            \"video_size\": len(video_bytes),\n            \"duration\": duration,\n            \"fps\": fps,\n            \"resolution\": resolution,\n        }\n    \n    async def _extract_resolution(self, video_bytes: bytes) -> str:\n        \"\"\"\n        Extract video resolution from video bytes.\n        \n        Returns:\n            str: Resolution in format \"WIDTHxHEIGHT\"\n        \"\"\"\n        # Implementation would use ffprobe or similar\n        # For now, return placeholder\n        return \"1024x576\"\n```\n\n## Model Configuration\n\n### Environment Variables\n\n```bash\n# Default models (optional)\nDEFAULT_TEXT_TO_VIDEO_MODEL=damo-vilab/text-to-video-ms-1.7b\nDEFAULT_IMAGE_TO_VIDEO_MODEL=stabilityai/stable-video-diffusion-img2vid\n\n# Fallback models for resilience\nFALLBACK_TEXT_TO_VIDEO_MODELS=cerspense/zeroscope_v2_576w,cerspense/zeroscope_v2_XL\nFALLBACK_IMAGE_TO_VIDEO_MODELS=stabilityai/stable-video-diffusion-img2vid-xt\n\n# Video constraints\nMAX_VIDEO_DURATION=30\nMAX_VIDEO_FILE_SIZE=524288000\nVIDEO_QUALITY_PRESET=medium\n```\n\n### Configuration Class\n\n```python\nclass Config:\n    \"\"\"Video configuration.\"\"\"\n    \n    # Default models\n    DEFAULT_TEXT_TO_VIDEO_MODEL = os.getenv(\n        \"DEFAULT_TEXT_TO_VIDEO_MODEL\",\n        \"damo-vilab/text-to-video-ms-1.7b\"\n    )\n    DEFAULT_IMAGE_TO_VIDEO_MODEL = os.getenv(\n        \"DEFAULT_IMAGE_TO_VIDEO_MODEL\",\n        \"stabilityai/stable-video-diffusion-img2vid\"\n    )\n    \n    # Fallback models\n    FALLBACK_TEXT_TO_VIDEO_MODELS = os.getenv(\n        \"FALLBACK_TEXT_TO_VIDEO_MODELS\",\n        \"cerspense/zeroscope_v2_576w,cerspense/zeroscope_v2_XL\"\n    ).split(\",\")\n    \n    FALLBACK_IMAGE_TO_VIDEO_MODELS = os.getenv(\n        \"FALLBACK_IMAGE_TO_VIDEO_MODELS\",\n        \"stabilityai/stable-video-diffusion-img2vid-xt\"\n    ).split(\",\")\n    \n    # Constraints\n    MAX_VIDEO_DURATION = int(os.getenv(\"MAX_VIDEO_DURATION\", 30))\n    MAX_VIDEO_FILE_SIZE = int(os.getenv(\"MAX_VIDEO_FILE_SIZE\", 524288000))\n    VIDEO_QUALITY_PRESET = os.getenv(\"VIDEO_QUALITY_PRESET\", \"medium\")\n```\n\n## Error Handling\n\n### Error Flow\n\n```python\n# Try primary model\ntry:\n    video = await video_service.generate_text_to_video(\n        prompt=\"A cat\",\n        model=\"primary-model\"\n    )\nexcept ModelNotFoundError:\n    # Try first fallback\n    try:\n        video = await video_service.generate_text_to_video(\n            prompt=\"A cat\",\n            model=Config.FALLBACK_TEXT_TO_VIDEO_MODELS[0]\n        )\n    except ModelNotFoundError:\n        # Try second fallback\n        video = await video_service.generate_text_to_video(\n            prompt=\"A cat\",\n            model=Config.FALLBACK_TEXT_TO_VIDEO_MODELS[1]\n        )\nexcept HuggingFaceAPIError as e:\n    # Retry with exponential backoff\n    video = await retry_with_backoff(\n        lambda: video_service.generate_text_to_video(\n            prompt=\"A cat\",\n            model=\"primary-model\"\n        )\n    )\n```\n\n## Performance Optimization\n\n### Caching Strategy\n\n1. **Model Caching**: Models cached in memory after first use\n2. **Video Caching**: Generated videos cached based on prompt hash\n3. **Metadata Caching**: Video metadata cached for quick retrieval\n\n### Resource Management\n\n1. **Memory Limits**: Monitor and limit model memory usage\n2. **Timeout Handling**: Set appropriate timeouts for long-running operations\n3. **Concurrent Requests**: Handle multiple requests efficiently\n\n## Testing Strategy\n\n### Unit Tests\n\n```python\n# Test model-agnostic behavior\ndef test_text_to_video_with_custom_model():\n    service = VideoService()\n    video = service.generate_text_to_video(\n        prompt=\"A cat\",\n        model=\"custom/model\"\n    )\n    assert video is not None\n\n# Test fallback behavior\ndef test_text_to_video_fallback():\n    service = VideoService()\n    video = service.generate_text_to_video(\n        prompt=\"A cat\",\n        model=\"invalid/model\"  # Should fallback\n    )\n    assert video is not None\n```\n\n## Future Enhancements\n\n1. **Streaming Generation**: Stream video generation progress\n2. **Batch Processing**: Generate multiple videos efficiently\n3. **Model Caching**: Cache models across requests\n4. **Performance Monitoring**: Track inference times and resource usage\n5. **Advanced Fallback**: Intelligent model selection based on performance\n
