# Video Integration Guide\n\nThis guide provides complete instructions for integrating video generation capabilities into your applications using the AI Platform.\n\n## Quick Start\n\n### Backend Integration\n\nThe video endpoints are automatically available once the backend is running. No additional setup required.\n\n**Text-to-Video Endpoint**:\n```bash\ncurl -X POST http://localhost:8000/api/video/text-to-video \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"A cat walking through a garden\",\n    \"duration\": 8,\n    \"fps\": 24\n  }' \\\n  --output video.mp4\n```\n\n**Image-to-Video Endpoint**:\n```bash\ncurl -X POST http://localhost:8000/api/video/image-to-video \\\n  -F \"image=@input_image.jpg\" \\\n  -F \"prompt=cinematic movement\" \\\n  -F \"fps=24\" \\\n  --output video.mp4\n```\n\n### Mobile Client Integration\n\nThe mobile client includes pre-built screens for video generation:\n\n1. **VideoGenerationScreen** - Text-to-video generation\n2. **ImageToVideoScreen** - Image-to-video generation\n\nBoth screens are integrated into the tab navigation and ready to use.\n\n## Backend Implementation Details\n\n### Service Layer\n\nThe `VideoService` class provides model-agnostic video generation:\n\n```python\nfrom app.services.video_service import get_video_service\n\nvideo_service = get_video_service()\n\n# Generate text-to-video\nvideo_data = await video_service.generate_text_to_video(\n    prompt=\"A beautiful sunset\",\n    model=\"damo-vilab/text-to-video-ms-1.7b\",\n    duration=8,\n    fps=24,\n    num_inference_steps=50\n)\n\n# Generate image-to-video\nvideo_data = await video_service.generate_image_to_video(\n    image_data=image_bytes,\n    model=\"stabilityai/stable-video-diffusion-img2vid\",\n    prompt=\"cinematic movement\",\n    duration=6,\n    fps=8\n)\n```\n\n### API Routers\n\nTwo routers handle video generation requests:\n\n**Text-to-Video Router**:\n```python\nfrom app.routers.video_router import router\n\n# Registered at: POST /api/video/text-to-video\n# Accepts: TextToVideoRequest (JSON)\n# Returns: MP4 video file (streaming)\n```\n\n**Image-to-Video Router**:\n```python\n# Registered at: POST /api/video/image-to-video\n# Accepts: Multipart form data with image and parameters\n# Returns: MP4 video file (streaming)\n```\n\n### Configuration\n\nConfigure video generation via environment variables:\n\n```bash\n# Default models\nDEFAULT_TEXT_TO_VIDEO_MODEL=damo-vilab/text-to-video-ms-1.7b\nDEFAULT_IMAGE_TO_VIDEO_MODEL=stabilityai/stable-video-diffusion-img2vid\n\n# Fallback models for resilience\nFALLBACK_TEXT_TO_VIDEO_MODELS=cerspense/zeroscope_v2_576w,cerspense/zeroscope_v2_XL\nFALLBACK_IMAGE_TO_VIDEO_MODELS=stabilityai/stable-video-diffusion-img2vid-xt\n\n# Video constraints\nMAX_VIDEO_DURATION=30\nMAX_VIDEO_FILE_SIZE=524288000\nVIDEO_QUALITY_PRESET=medium\n```\n\n## Mobile Client Implementation Details\n\n### API Client Methods\n\nThe API client includes video generation methods:\n\n```typescript\nimport { getAPIClient } from '../services/api';\n\nconst apiClient = getAPIClient();\n\n// Text-to-video\nconst videoBlob = await apiClient.generateTextToVideo({\n  prompt: \"A cat walking\",\n  duration: 8,\n  fps: 24,\n  num_inference_steps: 50,\n});\n\n// Image-to-video\nconst videoBlob = await apiClient.generateImageToVideo({\n  image: base64EncodedImage,\n  prompt: \"cinematic movement\",\n  fps: 24,\n});\n```\n\n### Caching\n\nGenerated videos are automatically cached:\n\n```typescript\nimport { getCacheService } from '../services/cache';\n\nconst cacheService = getCacheService();\n\n// Cache a video\nconst cacheKey = `video_${Date.now()}`;\nconst cachedUri = await cacheService.cacheVideo(cacheKey, videoBlob);\n\n// Retrieve cached video\nconst cachedVideo = await cacheService.getCachedVideo(cacheKey);\n```\n\n### Video Playback\n\nPlay cached videos using Expo Video component:\n\n```typescript\nimport { Video } from 'expo-av';\n\n<Video\n  source={{ uri: videoUri }}\n  rate={1.0}\n  volume={1.0}\n  isMuted={false}\n  resizeMode=\"cover\"\n  useNativeControls\n  style={{ width: '100%', height: 400 }}\n/>\n```\n\n## Advanced Usage\n\n### Custom Model Selection\n\nUse any HuggingFace video model by specifying the model ID:\n\n```bash\n# Using custom model\ncurl -X POST http://localhost:8000/api/video/text-to-video \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Your prompt\",\n    \"model\": \"your-org/your-custom-video-model\"\n  }'\n```\n\n### Batch Processing\n\nGenerate multiple videos efficiently:\n\n```python\nimport asyncio\nfrom app.services.video_service import get_video_service\n\nvideo_service = get_video_service()\n\nprompts = [\n    \"A cat walking\",\n    \"A dog running\",\n    \"A bird flying\"\n]\n\n# Generate videos concurrently\ntasks = [\n    video_service.generate_text_to_video(prompt=prompt)\n    for prompt in prompts\n]\nresults = await asyncio.gather(*tasks)\n```\n\n### Error Handling\n\nHandle video generation errors gracefully:\n\n```python\nfrom app.utils.exceptions import (\n    ValidationError,\n    ModelNotFoundError,\n    HuggingFaceAPIError,\n    ProcessingError,\n)\n\ntry:\n    video_data = await video_service.generate_text_to_video(\n        prompt=\"A cat\",\n        model=\"primary-model\"\n    )\nexcept ValidationError as e:\n    # Handle validation errors\n    print(f\"Validation error: {e.message}\")\nexcept ModelNotFoundError as e:\n    # Model not found - will automatically try fallback\n    print(f\"Model error: {e.message}\")\nexcept HuggingFaceAPIError as e:\n    # API error - will retry with backoff\n    print(f\"API error: {e.message}\")\nexcept ProcessingError as e:\n    # Processing error\n    print(f\"Processing error: {e.message}\")\n```\n\n## Integration Examples\n\n### Example 1: Simple Web Client\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Video Generation</title>\n</head>\n<body>\n    <h1>Generate Video from Text</h1>\n    \n    <input type=\"text\" id=\"prompt\" placeholder=\"Enter prompt\">\n    <button onclick=\"generateVideo()\">Generate</button>\n    \n    <video id=\"videoPlayer\" controls></video>\n    \n    <script>\n        async function generateVideo() {\n            const prompt = document.getElementById('prompt').value;\n            \n            try {\n                const response = await fetch('http://localhost:8000/api/video/text-to-video', {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json'\n                    },\n                    body: JSON.stringify({\n                        prompt: prompt,\n                        duration: 8,\n                        fps: 24\n                    })\n                });\n                \n                const blob = await response.blob();\n                const url = URL.createObjectURL(blob);\n                document.getElementById('videoPlayer').src = url;\n            } catch (error) {\n                alert('Error generating video: ' + error.message);\n            }\n        }\n    </script>\n</body>\n</html>\n```\n\n### Example 2: Python Client\n\n```python\nimport requests\nimport json\n\ndef generate_text_to_video(prompt, duration=8, fps=24):\n    \"\"\"Generate video from text prompt.\"\"\"\n    url = 'http://localhost:8000/api/video/text-to-video'\n    \n    payload = {\n        'prompt': prompt,\n        'duration': duration,\n        'fps': fps,\n        'num_inference_steps': 50\n    }\n    \n    response = requests.post(url, json=payload)\n    \n    if response.status_code == 200:\n        with open('output.mp4', 'wb') as f:\n            f.write(response.content)\n        print('Video saved to output.mp4')\n    else:\n        print(f'Error: {response.status_code}')\n        print(response.json())\n\n# Usage\ngenerate_text_to_video('A cat walking through a garden')\n```\n\n### Example 3: Node.js Client\n\n```javascript\nconst axios = require('axios');\nconst fs = require('fs');\n\nasync function generateTextToVideo(prompt, duration = 8, fps = 24) {\n  try {\n    const response = await axios.post(\n      'http://localhost:8000/api/video/text-to-video',\n      {\n        prompt,\n        duration,\n        fps,\n        num_inference_steps: 50\n      },\n      {\n        responseType: 'arraybuffer',\n        timeout: 600000 // 10 minutes\n      }\n    );\n\n    fs.writeFileSync('output.mp4', response.data);\n    console.log('Video saved to output.mp4');\n  } catch (error) {\n    console.error('Error:', error.message);\n  }\n}\n\n// Usage\ngenerateTe xtToVideo('A beautiful sunset over mountains');\n```\n\n## Performance Optimization\n\n### Caching Strategy\n\nImplement multi-level caching for optimal performance:\n\n1. **Memory Cache**: Fast access for frequently generated videos\n2. **Disk Cache**: Persistent storage for offline access\n3. **TTL**: Automatic expiration of old cached videos\n\n```typescript\nconst cacheService = getCacheService();\n\n// Cache configuration\nconst cacheConfig = {\n  maxMemoryItems: 50,\n  maxDiskSize: 100 * 1024 * 1024, // 100MB\n  ttl: 3600000, // 1 hour\n};\n\n// Cache video\nconst cachedUri = await cacheService.cacheVideo(key, videoBlob);\n```\n\n### Model Selection\n\nChoose models based on your performance requirements:\n\n| Model | Speed | Quality | VRAM |\n|-------|-------|---------|------|\n| `damo-vilab/text-to-video-ms-1.7b` | Fast | Good | 4GB |\n| `cerspense/zeroscope_v2_576w` | Medium | Better | 6GB |\n| `cerspense/zeroscope_v2_XL` | Slow | Best | 8GB |\n\n### Timeout Configuration\n\nSet appropriate timeouts for long-running operations:\n\n```python\n# Backend\nREQUEST_TIMEOUT = 600  # 10 minutes\n\n# Mobile\nconst timeout = 600000; // 10 minutes\n```\n\n## Troubleshooting\n\n### Video Generation Times Out\n\n**Issue**: Request exceeds timeout\n\n**Solution**:\n1. Increase `REQUEST_TIMEOUT` in configuration\n2. Use a faster model\n3. Reduce video duration or resolution\n4. Reduce `num_inference_steps`\n\n### Out of Memory\n\n**Issue**: CUDA out of memory error\n\n**Solution**:\n1. Use a smaller model\n2. Reduce batch size\n3. Upgrade GPU memory\n4. Use CPU inference (slower)\n\n### Model Not Found\n\n**Issue**: HuggingFace model not found\n\n**Solution**:\n1. Verify model name is correct\n2. Check model exists on HuggingFace Hub\n3. Verify HF_API_KEY has access\n4. Use fallback model\n\n### Poor Video Quality\n\n**Issue**: Generated video quality is low\n\n**Solution**:\n1. Increase `num_inference_steps`\n2. Use a higher-quality model\n3. Improve prompt description\n4. Avoid conflicting negative prompts\n\n## Best Practices\n\n### Prompt Engineering\n\nWrite effective prompts for better results:\n\n**Good prompts** are specific and descriptive:\n- \"A golden retriever running through a field of sunflowers at sunset\"\n- \"Cinematic shot of a spaceship entering Earth's atmosphere\"\n- \"A person dancing to upbeat music in a modern living room\"\n\n**Poor prompts** are vague:\n- \"A dog\"\n- \"A spaceship\"\n- \"A person\"\n\n### Error Handling\n\nImplement robust error handling:\n\n```typescript\ntry {\n  const videoBlob = await apiClient.generateTextToVideo(params);\n  // Success\n} catch (error) {\n  if (error.response?.status === 422) {\n    // Validation error - check parameters\n  } else if (error.response?.status === 404) {\n    // Model not found - try alternative\n  } else if (error.response?.status === 502) {\n    // API error - retry\n  } else if (error.code === 'ECONNABORTED') {\n    // Timeout - increase timeout\n  }\n}\n```\n\n### Rate Limiting\n\nImplement rate limiting to prevent abuse:\n\n```python\nfrom fastapi_limiter import FastAPILimiter\nfrom fastapi_limiter.util import get_remote_address\n\n@limiter.limit(\"5/minute\")\nasync def text_to_video(request: TextToVideoRequest):\n    # Implementation\n    pass\n```\n\n## Monitoring\n\nMonitor video generation performance:\n\n```python\nimport time\n\nstart_time = time.time()\nvideo_data = await video_service.generate_text_to_video(...)\ngeneration_time = time.time() - start_time\n\nlogger.info(\n    \"Video generated\",\n    extra={\n        \"generation_time\": generation_time,\n        \"video_size\": len(video_data[\"video_bytes\"]),\n        \"model\": video_data[\"model\"],\n    }\n)\n```\n\n## References\n\n- [HuggingFace Video Models](https://huggingface.co/models?pipeline_tag=text-to-video)\n- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n- [React Native Documentation](https://reactnative.dev/)\n- [Expo Video Component](https://docs.expo.dev/versions/latest/sdk/video/)\n
