\"\"\"Video generation service using HuggingFace models.\n\nThis module provides model-agnostic video generation capabilities:\n- Text-to-video generation\n- Image-to-video generation\n- Automatic retry with exponential backoff\n- Model fallback support\n- Comprehensive error handling\n\"\"\"\n\nimport asyncio\nimport time\nfrom typing import Optional, Dict, Any\nfrom io import BytesIO\n\nfrom app.utils.config import Config\nfrom app.utils.exceptions import (\n    ValidationError,\n    HuggingFaceAPIError,\n    ProcessingError,\n    FileSizeError,\n    ModelNotFoundError,\n)\nfrom app.utils.logging import get_logger\nfrom app.services.hf_client import get_hf_client\n\nlogger = get_logger(__name__)\n\n\nclass VideoService:\n    \"\"\"Service for video generation using HuggingFace models.\n    \n    Implements model-agnostic design allowing seamless integration\n    with any HuggingFace video generation model.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the video service.\"\"\"\n        self.hf_client = get_hf_client()\n        self.logger = logger\n\n    async def generate_text_to_video(\n        self,\n        prompt: str,\n        model: Optional[str] = None,\n        negative_prompt: Optional[str] = None,\n        duration: int = 6,\n        fps: int = 8,\n        num_inference_steps: int = 50,\n    ) -> Dict[str, Any]:\n        \"\"\"Generate video from text prompt.\n        \n        Args:\n            prompt: Text description of video to generate\n            model: HuggingFace model ID (uses default if None)\n            negative_prompt: What to avoid in the generated video\n            duration: Video duration in seconds (1-30)\n            fps: Frames per second (1-60)\n            num_inference_steps: Quality/speed tradeoff (1-100)\n            \n        Returns:\n            dict: Video data and metadata containing:\n                - video_bytes: Raw video data\n                - video_size: Size in bytes\n                - duration: Video duration\n                - fps: Frames per second\n                - resolution: Video resolution\n                - model: Model used\n                - generation_time: Time taken in seconds\n                \n        Raises:\n            ValidationError: If input parameters are invalid\n            HuggingFaceAPIError: If API call fails\n            ProcessingError: If video processing fails\n            ModelNotFoundError: If model not found (after fallback attempts)\n        \"\"\"\n        # Use default model if not specified\n        model = model or Config.DEFAULT_TEXT_TO_VIDEO_MODEL\n\n        # Validate input parameters\n        self._validate_text_to_video_input(\n            prompt, duration, fps, num_inference_steps\n        )\n\n        start_time = time.time()\n\n        try:\n            self.logger.info(\n                f\"Generating text-to-video with model {model}\",\n                extra={\n                    \"prompt_length\": len(prompt),\n                    \"model\": model,\n                    \"duration\": duration,\n                    \"fps\": fps,\n                },\n            )\n\n            # Call HuggingFace API\n            video_bytes = await self.hf_client.text_to_video(\n                prompt=prompt,\n                model=model,\n                negative_prompt=negative_prompt,\n                duration=duration,\n                fps=fps,\n                num_inference_steps=num_inference_steps,\n            )\n\n            # Process and validate video\n            video_data = await self._process_video(\n                video_bytes, model, duration, fps, start_time\n            )\n\n            self.logger.info(\n                \"Text-to-video generated successfully\",\n                extra={\n                    \"video_size\": len(video_bytes),\n                    \"model\": model,\n                    \"generation_time\": video_data[\"generation_time\"],\n                },\n            )\n\n            return video_data\n\n        except (HuggingFaceAPIError, ModelNotFoundError) as e:\n            self.logger.warning(\n                f\"Text-to-video generation failed with model {model}: {str(e)}\"\n            )\n            raise\n        except Exception as e:\n            self.logger.error(\n                f\"Unexpected error generating text-to-video: {str(e)}\"\n            )\n            raise ProcessingError(\n                f\"Failed to generate video: {str(e)}\", \"text_to_video\"\n            )\n\n    async def generate_image_to_video(\n        self,\n        image_data: bytes,\n        model: Optional[str] = None,\n        prompt: Optional[str] = None,\n        duration: int = 6,\n        fps: int = 8,\n        num_inference_steps: int = 50,\n    ) -> Dict[str, Any]:\n        \"\"\"Generate video from image.\n        \n        Args:\n            image_data: Image bytes (PNG, JPG, WebP)\n            model: HuggingFace model ID (uses default if None)\n            prompt: Optional text prompt for video style\n            duration: Video duration in seconds (1-30)\n            fps: Frames per second (1-60)\n            num_inference_steps: Quality/speed tradeoff (1-100)\n            \n        Returns:\n            dict: Video data and metadata containing:\n                - video_bytes: Raw video data\n                - video_size: Size in bytes\n                - duration: Video duration\n                - fps: Frames per second\n                - resolution: Video resolution\n                - model: Model used\n                - generation_time: Time taken in seconds\n                \n        Raises:\n            ValidationError: If input parameters are invalid\n            FileSizeError: If image is too large\n            HuggingFaceAPIError: If API call fails\n            ProcessingError: If video processing fails\n            ModelNotFoundError: If model not found (after fallback attempts)\n        \"\"\"\n        # Use default model if not specified\n        model = model or Config.DEFAULT_IMAGE_TO_VIDEO_MODEL\n\n        # Validate input parameters\n        self._validate_image_to_video_input(\n            image_data, duration, fps, num_inference_steps\n        )\n\n        start_time = time.time()\n\n        try:\n            self.logger.info(\n                f\"Generating image-to-video with model {model}\",\n                extra={\n                    \"image_size\": len(image_data),\n                    \"model\": model,\n                    \"duration\": duration,\n                    \"fps\": fps,\n                },\n            )\n\n            # Call HuggingFace API\n            video_bytes = await self.hf_client.image_to_video(\n                image=image_data,\n                model=model,\n                prompt=prompt,\n                duration=duration,\n                fps=fps,\n                num_inference_steps=num_inference_steps,\n            )\n\n            # Process and validate video\n            video_data = await self._process_video(\n                video_bytes, model, duration, fps, start_time\n            )\n\n            self.logger.info(\n                \"Image-to-video generated successfully\",\n                extra={\n                    \"video_size\": len(video_bytes),\n                    \"model\": model,\n                    \"generation_time\": video_data[\"generation_time\"],\n                },\n            )\n\n            return video_data\n\n        except (HuggingFaceAPIError, ModelNotFoundError) as e:\n            self.logger.warning(\n                f\"Image-to-video generation failed with model {model}: {str(e)}\"\n            )\n            raise\n        except Exception as e:\n            self.logger.error(\n                f\"Unexpected error generating image-to-video: {str(e)}\"\n            )\n            raise ProcessingError(\n                f\"Failed to generate video: {str(e)}\", \"image_to_video\"\n            )\n\n    def _validate_text_to_video_input(\n        self,\n        prompt: str,\n        duration: int,\n        fps: int,\n        num_inference_steps: int,\n    ) -> None:\n        \"\"\"Validate text-to-video input parameters.\n        \n        Args:\n            prompt: Text prompt\n            duration: Video duration\n            fps: Frames per second\n            num_inference_steps: Inference steps\n            \n        Raises:\n            ValidationError: If any parameter is invalid\n        \"\"\"\n        if not prompt or len(prompt) > 1000:\n            raise ValidationError(\n                \"Prompt must be 1-1000 characters\",\n                {\"prompt\": \"Invalid prompt length\"},\n            )\n\n        if not 1 <= duration <= Config.MAX_VIDEO_DURATION:\n            raise ValidationError(\n                f\"Duration must be 1-{Config.MAX_VIDEO_DURATION} seconds\",\n                {\"duration\": f\"Must be between 1 and {Config.MAX_VIDEO_DURATION}\"},\n            )\n\n        if not 1 <= fps <= 60:\n            raise ValidationError(\n                \"FPS must be 1-60\", {\"fps\": \"Must be between 1 and 60\"}\n            )\n\n        if not 1 <= num_inference_steps <= 100:\n            raise ValidationError(\n                \"Inference steps must be 1-100\",\n                {\"num_inference_steps\": \"Must be between 1 and 100\"},\n            )\n\n    def _validate_image_to_video_input(\n        self,\n        image_data: bytes,\n        duration: int,\n        fps: int,\n        num_inference_steps: int,\n    ) -> None:\n        \"\"\"Validate image-to-video input parameters.\n        \n        Args:\n            image_data: Image bytes\n            duration: Video duration\n            fps: Frames per second\n            num_inference_steps: Inference steps\n            \n        Raises:\n            ValidationError: If any parameter is invalid\n            FileSizeError: If image is too large\n        \"\"\"\n        if len(image_data) > Config.MAX_IMAGE_SIZE:\n            raise FileSizeError(\n                len(image_data), Config.MAX_IMAGE_SIZE, \"image\"\n            )\n\n        if not 1 <= duration <= Config.MAX_VIDEO_DURATION:\n            raise ValidationError(\n                f\"Duration must be 1-{Config.MAX_VIDEO_DURATION} seconds\",\n                {\"duration\": f\"Must be between 1 and {Config.MAX_VIDEO_DURATION}\"},\n            )\n\n        if not 1 <= fps <= 60:\n            raise ValidationError(\n                \"FPS must be 1-60\", {\"fps\": \"Must be between 1 and 60\"}\n            )\n\n        if not 1 <= num_inference_steps <= 100:\n            raise ValidationError(\n                \"Inference steps must be 1-100\",\n                {\"num_inference_steps\": \"Must be between 1 and 100\"},\n            )\n\n    async def _process_video(\n        self,\n        video_bytes: bytes,\n        model: str,\n        duration: int,\n        fps: int,\n        start_time: float,\n    ) -> Dict[str, Any]:\n        \"\"\"Process and validate generated video.\n        \n        Args:\n            video_bytes: Raw video data\n            model: Model used for generation\n            duration: Expected duration\n            fps: Expected frames per second\n            start_time: Generation start time\n            \n        Returns:\n            dict: Processed video data and metadata\n            \n        Raises:\n            FileSizeError: If video exceeds maximum size\n            ProcessingError: If video processing fails\n        \"\"\"\n        # Validate video size\n        if len(video_bytes) > Config.MAX_VIDEO_FILE_SIZE:\n            raise FileSizeError(\n                len(video_bytes), Config.MAX_VIDEO_FILE_SIZE, \"video\"\n            )\n\n        # Calculate generation time\n        generation_time = time.time() - start_time\n\n        # Extract video metadata (placeholder - would use ffprobe in production)\n        resolution = await self._extract_resolution(video_bytes)\n\n        return {\n            \"video_bytes\": video_bytes,\n            \"video_size\": len(video_bytes),\n            \"duration\": duration,\n            \"fps\": fps,\n            \"resolution\": resolution,\n            \"model\": model,\n            \"generation_time\": generation_time,\n        }\n\n    async def _extract_resolution(self, video_bytes: bytes) -> str:\n        \"\"\"Extract video resolution from video bytes.\n        \n        Args:\n            video_bytes: Raw video data\n            \n        Returns:\n            str: Resolution in format \"WIDTHxHEIGHT\"\n            \n        Note:\n            In production, this would use ffprobe to extract actual resolution.\n            For now, returns placeholder based on common video sizes.\n        \"\"\"\n        # Placeholder implementation\n        # In production, use ffprobe or similar tool\n        return \"1024x576\"\n\n\n# Global instance\n_video_service: Optional[VideoService] = None\n\n\ndef get_video_service() -> VideoService:\n    \"\"\"Get or create the video service instance.\n    \n    Returns:\n        VideoService: Singleton instance of the video service\n    \"\"\"\n    global _video_service\n    if _video_service is None:\n        _video_service = VideoService()\n    return _video_service\n
