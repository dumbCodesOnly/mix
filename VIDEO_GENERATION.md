# Video Generation Guide\n\n## Overview\n\nThe AI Platform now supports video generation through HuggingFace's video AI models. The implementation follows a **model-agnostic design pattern**, allowing seamless integration with any HuggingFace video model without code changes. This guide covers text-to-video and image-to-video generation capabilities.\n\n## Supported Video Models\n\nThe platform supports any HuggingFace video generation model, including but not limited to:\n\n### Text-to-Video Models\n\n| Model | Organization | Description | Input |\n|-------|---------------|-------------|-------|\n| `damo-vilab/text-to-video-ms-1.7b` | Alibaba DAMO | Lightweight text-to-video model | Text prompt |\n| `cerspense/zeroscope_v2_576w` | Cerspense | Fast video generation | Text prompt |\n| `cerspense/zeroscope_v2_XL` | Cerspense | High-quality video generation | Text prompt |\n| `stabilityai/stable-video-diffusion-img2vid` | Stability AI | Image-to-video model | Image + prompt |\n| `runwayml/stable-diffusion-v1-5` | Runway ML | Video generation variant | Text prompt |\n\n### Image-to-Video Models\n\n| Model | Organization | Description | Input |\n|-------|---------------|-------------|-------|\n| `stabilityai/stable-video-diffusion-img2vid` | Stability AI | Official image-to-video | Image |\n| `stabilityai/stable-video-diffusion-img2vid-xt` | Stability AI | Extended version | Image |\n| `damo-vilab/animatediff-text-to-video` | Alibaba DAMO | Animation from images | Image + prompt |\n\n**Note**: All models are model-agnostic. You can use any HuggingFace video model by specifying its model ID in requests.\n\n## Architecture\n\n### Model-Agnostic Design\n\nThe video service layer is designed to work with any HuggingFace video model:\n\n```\nClient Request\n    ↓\n[Specify Model Name]\n    ↓\n[Video Service]\n    ├─ Validate Input (prompt/image)\n    ├─ Call HF Client with Model Name\n    ├─ Handle Retry Logic\n    └─ Return Video Bytes\n    ↓\n[Response]\n```\n\n### Service Architecture\n\n```python\nclass VideoService:\n    def generate_text_to_video(\n        self,\n        prompt: str,\n        model: Optional[str] = None,  # Model-agnostic\n        negative_prompt: Optional[str] = None,\n        duration: int = 6,\n        fps: int = 8,\n        num_inference_steps: int = 50,\n    ) -> dict:\n        # Uses default model if not specified\n        # Calls HF API with specified model\n        # Returns video bytes and metadata\n\n    def generate_image_to_video(\n        self,\n        image_data: bytes,\n        model: Optional[str] = None,  # Model-agnostic\n        prompt: Optional[str] = None,\n        duration: int = 6,\n        fps: int = 8,\n        num_inference_steps: int = 50,\n    ) -> dict:\n        # Uses default model if not specified\n        # Processes image and calls HF API\n        # Returns video bytes and metadata\n```\n\n## API Endpoints\n\n### Text-to-Video Generation\n\n**Endpoint**: `POST /api/video/text-to-video`\n\n**Request**:\n```json\n{\n  \"prompt\": \"A cat walking through a garden\",\n  \"model\": \"damo-vilab/text-to-video-ms-1.7b\",\n  \"negative_prompt\": \"blurry, distorted\",\n  \"duration\": 6,\n  \"fps\": 8,\n  \"num_inference_steps\": 50\n}\n```\n\n**Response**: MP4 video binary data\n\n**Parameters**:\n\n| Parameter | Type | Required | Default | Description |\n|-----------|------|----------|---------|-------------|\n| `prompt` | string | Yes | - | Text description of video |\n| `model` | string | No | `damo-vilab/text-to-video-ms-1.7b` | HuggingFace model ID |\n| `negative_prompt` | string | No | - | What to avoid in video |\n| `duration` | integer | No | 6 | Video duration in seconds |\n| `fps` | integer | No | 8 | Frames per second |\n| `num_inference_steps` | integer | No | 50 | Quality/speed tradeoff |\n\n**Example with curl**:\n```bash\ncurl -X POST http://localhost:8000/api/video/text-to-video \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"A beautiful sunset over mountains\",\n    \"duration\": 8,\n    \"fps\": 24\n  }' \\\n  --output generated_video.mp4\n```\n\n### Image-to-Video Generation\n\n**Endpoint**: `POST /api/video/image-to-video`\n\n**Request** (multipart form data):\n```\nimage: <binary image file>\nmodel: stabilityai/stable-video-diffusion-img2vid (optional)\nprompt: Optional text prompt for video style (optional)\nduration: 6 (optional)\nfps: 8 (optional)\nnum_inference_steps: 50 (optional)\n```\n\n**Response**: MP4 video binary data\n\n**Parameters**:\n\n| Parameter | Type | Required | Default | Description |\n|-----------|------|----------|---------|-------------|\n| `image` | file | Yes | - | Input image file (PNG, JPG) |\n| `model` | string | No | `stabilityai/stable-video-diffusion-img2vid` | HuggingFace model ID |\n| `prompt` | string | No | - | Optional text prompt for style |\n| `duration` | integer | No | 6 | Video duration in seconds |\n| `fps` | integer | No | 8 | Frames per second |\n| `num_inference_steps` | integer | No | 50 | Quality/speed tradeoff |\n\n**Example with curl**:\n```bash\ncurl -X POST http://localhost:8000/api/video/image-to-video \\\n  -F \"image=@input_image.jpg\" \\\n  -F \"prompt=cinematic camera movement\" \\\n  -F \"fps=24\" \\\n  --output generated_video.mp4\n```\n\n## Configuration\n\n### Environment Variables\n\nAdd these to your `.env` file:\n\n```bash\n# Default video models (optional - uses fallback if not specified)\nDEFAULT_TEXT_TO_VIDEO_MODEL=damo-vilab/text-to-video-ms-1.7b\nDEFAULT_IMAGE_TO_VIDEO_MODEL=stabilityai/stable-video-diffusion-img2vid\n\n# Video processing configuration\nMAX_VIDEO_DURATION=30              # Maximum video duration in seconds\nMAX_VIDEO_FILE_SIZE=524288000      # 500MB max file size\nVIDEO_QUALITY_PRESET=medium        # low, medium, high\nVIDEO_FRAME_RATE=8                 # Default frames per second\nVIDEO_INFERENCE_STEPS=50           # Default inference steps\n```\n\n### Configuration in Code\n\n```python\nfrom app.utils.config import Config\n\n# Access video configuration\ndefault_text_to_video = Config.DEFAULT_TEXT_TO_VIDEO_MODEL\ndefault_image_to_video = Config.DEFAULT_IMAGE_TO_VIDEO_MODEL\nmax_duration = Config.MAX_VIDEO_DURATION\n```\n\n## Model Selection\n\n### Choosing the Right Model\n\n**For Speed** (Fast inference):\n- `damo-vilab/text-to-video-ms-1.7b` - Lightweight, fast\n- `cerspense/zeroscope_v2_576w` - Compact, suitable for real-time\n\n**For Quality** (Better results):\n- `cerspense/zeroscope_v2_XL` - Higher quality output\n- `stabilityai/stable-video-diffusion-img2vid-xt` - Extended version\n\n**For Image-to-Video**:\n- `stabilityai/stable-video-diffusion-img2vid` - Recommended\n- `stabilityai/stable-video-diffusion-img2vid-xt` - Extended version\n\n### Specifying Models in Requests\n\n```bash\n# Use default model\ncurl -X POST http://localhost:8000/api/video/text-to-video \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": \"A cat\"}'\n\n# Use specific model\ncurl -X POST http://localhost:8000/api/video/text-to-video \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"A cat\",\n    \"model\": \"cerspense/zeroscope_v2_XL\"\n  }'\n```\n\n## Performance Characteristics\n\n### Inference Time\n\n| Model | Resolution | Duration | Inference Time |\n|-------|------------|----------|----------------|\n| `damo-vilab/text-to-video-ms-1.7b` | 576x320 | 6s | 30-60 seconds |\n| `cerspense/zeroscope_v2_576w` | 576x320 | 6s | 45-90 seconds |\n| `cerspense/zeroscope_v2_XL` | 1024x576 | 6s | 120-180 seconds |\n| `stabilityai/stable-video-diffusion-img2vid` | 1024x576 | 6s | 60-120 seconds |\n\n### Resource Requirements\n\n| Model | VRAM | RAM | Disk |\n|-------|------|-----|------|\n| `damo-vilab/text-to-video-ms-1.7b` | 4GB | 8GB | 5GB |\n| `cerspense/zeroscope_v2_576w` | 6GB | 12GB | 8GB |\n| `cerspense/zeroscope_v2_XL` | 8GB | 16GB | 10GB |\n| `stabilityai/stable-video-diffusion-img2vid` | 10GB | 20GB | 15GB |\n\n## Error Handling\n\nThe video service implements comprehensive error handling:\n\n```python\n# Model not found - automatic fallback\nif model_not_available:\n    use_fallback_model()\n\n# Retry on transient failures\nretry_with_exponential_backoff()\n\n# Validation errors\nif not valid_prompt:\n    raise ValidationError(\"Invalid prompt\")\n\nif image_too_large:\n    raise FileSizeError(\"Image exceeds maximum size\")\n```\n\n### Error Responses\n\n```json\n{\n  \"error\": \"model_not_found\",\n  \"message\": \"Model 'invalid/model' not found on HuggingFace Hub\",\n  \"details\": {\n    \"requested_model\": \"invalid/model\",\n    \"fallback_model\": \"damo-vilab/text-to-video-ms-1.7b\"\n  },\n  \"timestamp\": \"2025-12-22T10:30:00Z\"\n}\n```\n\n## Mobile Client Integration\n\n### Video Generation Screen\n\n```typescript\nimport { getAPIClient } from '../services/api';\nimport { getCacheService } from '../services/cache';\n\n// Text-to-video\nconst videoBlob = await apiClient.generateTextToVideo({\n  prompt: \"A cat walking through a garden\",\n  duration: 8,\n  fps: 24,\n});\n\n// Image-to-video\nconst formData = new FormData();\nformData.append('image', imageBlob);\nformData.append('prompt', 'cinematic movement');\nformData.append('fps', '24');\n\nconst videoBlob = await apiClient.generateImageToVideo(formData);\n```\n\n### Video Caching\n\n```typescript\n// Cache generated videos\nconst cacheService = getCacheService();\nconst videoCachePath = await cacheService.cacheVideo(\n  prompt,\n  videoBlob\n);\n\n// Retrieve cached video\nconst cachedVideo = await cacheService.getCachedVideo(prompt);\n```\n\n### Video Playback\n\n```typescript\nimport { Video } from 'expo-av';\n\n<Video\n  source={{ uri: videoCachePath }}\n  rate={1.0}\n  volume={1.0}\n  isMuted={false}\n  resizeMode=\"cover\"\n  useNativeControls\n  style={{ width: '100%', height: 400 }}\n/>\n```\n\n## Advanced Usage\n\n### Batch Video Generation\n\nGenerate multiple videos sequentially:\n\n```python\nprompts = [\n    \"A cat walking\",\n    \"A dog running\",\n    \"A bird flying\"\n]\n\nfor prompt in prompts:\n    video = await video_service.generate_text_to_video(\n        prompt=prompt,\n        model=\"damo-vilab/text-to-video-ms-1.7b\"\n    )\n    save_video(video, f\"{prompt}.mp4\")\n```\n\n### Custom Model Configuration\n\nUse any HuggingFace video model:\n\n```bash\ncurl -X POST http://localhost:8000/api/video/text-to-video \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Your prompt\",\n    \"model\": \"your-org/your-custom-video-model\"\n  }'\n```\n\n### Retry and Fallback\n\nAutomatic retry with fallback models:\n\n```python\n# Primary model fails → tries fallback model\ntry:\n    video = await video_service.generate_text_to_video(\n        prompt=\"A cat\",\n        model=\"primary-model\"\n    )\nexcept ModelNotFoundError:\n    # Automatically tries fallback\n    video = await video_service.generate_text_to_video(\n        prompt=\"A cat\",\n        model=\"fallback-model\"\n    )\n```\n\n## Troubleshooting\n\n### Video Generation Times Out\n\n**Issue**: Request takes too long\n\n**Solution**:\n1. Reduce `num_inference_steps` (faster but lower quality)\n2. Use a faster model like `damo-vilab/text-to-video-ms-1.7b`\n3. Reduce video duration\n4. Increase `REQUEST_TIMEOUT` in configuration\n\n### Out of Memory\n\n**Issue**: CUDA out of memory error\n\n**Solution**:\n1. Use a smaller model\n2. Reduce video resolution\n3. Reduce batch size\n4. Upgrade GPU memory\n\n### Model Not Found\n\n**Issue**: HuggingFace model not found\n\n**Solution**:\n1. Verify model name is correct\n2. Check model exists on HuggingFace Hub\n3. Ensure HF_API_KEY has access\n4. Use fallback model\n\n### Poor Video Quality\n\n**Issue**: Generated video quality is low\n\n**Solution**:\n1. Increase `num_inference_steps` (slower but better quality)\n2. Use a higher-quality model\n3. Improve prompt description\n4. Avoid negative prompts that conflict with intent\n\n## Best Practices\n\n### Prompt Engineering\n\n**Good prompts** are specific and descriptive:\n```\n✓ \"A golden retriever running through a field of sunflowers at sunset\"\n✓ \"Cinematic shot of a spaceship entering Earth's atmosphere\"\n✓ \"A person dancing to upbeat music in a modern living room\"\n```\n\n**Poor prompts** are vague:\n```\n✗ \"A dog\"\n✗ \"A spaceship\"\n✗ \"A person\"\n```\n\n### Performance Optimization\n\n1. **Cache Results**: Store generated videos to avoid regeneration\n2. **Use Appropriate Models**: Match model to quality/speed needs\n3. **Batch Processing**: Generate multiple videos efficiently\n4. **Monitor Resources**: Track GPU/memory usage\n\n### Security\n\n1. **Validate Input**: Check prompt length and content\n2. **Rate Limiting**: Prevent abuse with usage quotas\n3. **File Size Limits**: Enforce maximum video file sizes\n4. **API Key Protection**: Never expose HuggingFace API key\n\n## Future Enhancements\n\n1. **Video Editing**: Combine multiple videos\n2. **Frame Interpolation**: Increase video smoothness\n3. **Style Transfer**: Apply artistic styles to videos\n4. **Multi-Model Ensemble**: Combine outputs from multiple models\n5. **Streaming Generation**: Stream video generation progress\n6. **Video Upscaling**: Enhance video resolution\n\n## References\n\n- [HuggingFace Video Models](https://huggingface.co/models?pipeline_tag=text-to-video)\n- [Damo-VILAB Text-to-Video](https://huggingface.co/damo-vilab/text-to-video-ms-1.7b)\n- [Zeroscope Video Models](https://huggingface.co/cerspense)\n- [Stability AI Video Models](https://huggingface.co/stabilityai)\n- [HuggingFace Inference API](https://huggingface.co/docs/api-inference/index)\n
