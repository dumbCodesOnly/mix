# Architecture Documentation\n\n## System Overview\n\nThe AI Platform is a distributed system consisting of:\n\n1. **Backend API** - FastAPI service providing AI model access\n2. **Mobile Client** - React Native app for user interaction\n3. **HuggingFace Hub** - External AI model provider\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    Mobile Client (React Native)             │\n│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │\n│  │ Image Gen    │  │ Image Edit   │  │ TTS          │      │\n│  └──────────────┘  └──────────────┘  └──────────────┘      │\n│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │\n│  │ STT          │  │ Chat (LLM)   │  │ Embeddings   │      │\n│  └──────────────┘  └──────────────┘  └──────────────┘      │\n└─────────────────────────────────────────────────────────────┘\n                           ↓ HTTP/REST\n┌─────────────────────────────────────────────────────────────┐\n│              Backend API (FastAPI + Python)                 │\n│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │\n│  │ Image Router │  │ TTS Router   │  │ STT Router   │      │\n│  └──────────────┘  └──────────────┘  └──────────────┘      │\n│  ┌──────────────┐  ┌──────────────┐                         │\n│  │ LLM Router   │  │ Embedding    │                         │\n│  │              │  │ Router       │                         │\n│  └──────────────┘  └──────────────┘                         │\n│                                                              │\n│  ┌──────────────────────────────────────────────────────┐  │\n│  │              Service Layer                           │  │\n│  │  ┌──────────────┐  ┌──────────────┐                │  │\n│  │  │ Image Service│  │ TTS Service  │                │  │\n│  │  └──────────────┘  └──────────────┘                │  │\n│  │  ┌──────────────┐  ┌──────────────┐                │  │\n│  │  │ STT Service  │  │ LLM Service  │                │  │\n│  │  └──────────────┘  └──────────────┘                │  │\n│  │  ┌──────────────┐                                  │  │\n│  │  │ Embedding    │                                  │  │\n│  │  │ Service      │                                  │  │\n│  │  └──────────────┘                                  │  │\n│  └──────────────────────────────────────────────────────┘  │\n│                                                              │\n│  ┌──────────────────────────────────────────────────────┐  │\n│  │         HuggingFace Client (hf_client.py)           │  │\n│  │  - Retry Logic (Exponential Backoff)                │  │\n│  │  - Model Fallback Support                           │  │\n│  │  - Error Handling                                   │  │\n│  │  - Request/Response Logging                         │  │\n│  └──────────────────────────────────────────────────────┘  │\n└─────────────────────────────────────────────────────────────┘\n                           ↓ HTTPS\n┌─────────────────────────────────────────────────────────────┐\n│              HuggingFace Inference API                       │\n│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │\n│  │ Image Models │  │ Audio Models │  │ Text Models  │      │\n│  └──────────────┘  └──────────────┘  └──────────────┘      │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Backend Architecture\n\n### Directory Structure\n\n```\napp/\n├── main.py                    # FastAPI application entry point\n├── routers/                   # API endpoint definitions\n│   ├── health_router.py      # Health check endpoint\n│   ├── image_router.py       # Image generation/editing\n│   ├── tts_router.py         # Text-to-speech\n│   ├── stt_router.py         # Speech-to-text\n│   ├── llm_router.py         # Language model\n│   └── embedding_router.py   # Embeddings\n├── services/                  # Business logic layer\n│   ├── hf_client.py          # HuggingFace API wrapper\n│   ├── image_service.py      # Image generation/editing\n│   ├── tts_service.py        # Text-to-speech\n│   ├── stt_service.py        # Speech-to-text\n│   ├── llm_service.py        # Language model\n│   └── embedding_service.py  # Embeddings\n└── utils/                     # Utility modules\n    ├── config.py             # Configuration management\n    ├── exceptions.py         # Custom exceptions\n    ├── logging.py            # Logging setup\n    ├── validation.py         # Pydantic models\n    └── retry.py              # Retry logic\n```\n\n### Request Flow\n\n1. **Client Request** → Mobile app sends HTTP request to backend\n2. **Router** → FastAPI router validates request and extracts parameters\n3. **Service** → Business logic layer processes request\n4. **HF Client** → Calls HuggingFace API with retry logic\n5. **Response** → Result returned to client with appropriate MIME type\n\n### Error Handling\n\n```\nRequest\n   ↓\n[Validation Error?] → 422 Unprocessable Entity\n   ↓ No\n[Service Processing]\n   ↓\n[HF API Error?] → Retry with Backoff\n   ↓ Success or Max Retries\n[Fallback Model?] → Try Alternative Model\n   ↓ Success or All Failed\n[Processing Error?] → 500 Internal Server Error\n   ↓ No\n[Success] → Return Result with Appropriate Status Code\n```\n\n### Retry Logic\n\nThe HuggingFace client implements exponential backoff retry:\n\n```\nAttempt 1: Immediate\n   ↓ Fail\nWait 1s\nAttempt 2: After 1 second\n   ↓ Fail\nWait 2s\nAttempt 3: After 2 seconds\n   ↓ Fail\nWait 4s\nAttempt 4: After 4 seconds\n   ↓ Fail or Success\n```\n\n## Mobile Client Architecture\n\n### Directory Structure\n\n```\nmobile_client/\n├── src/\n│   ├── screens/               # Screen components\n│   │   ├── HomeScreen.tsx\n│   │   ├── ImageGenerationScreen.tsx\n│   │   ├── ImageEditingScreen.tsx\n│   │   ├── TTSScreen.tsx\n│   │   ├── STTScreen.tsx\n│   │   ├── ChatScreen.tsx\n│   │   └── EmbeddingsScreen.tsx\n│   ├── components/            # Reusable components\n│   ├── services/              # Business logic\n│   │   ├── api.ts            # API client\n│   │   ├── cache.ts          # Image caching\n│   │   └── storage.ts        # Local storage\n│   ├── utils/                 # Utilities\n│   │   ├── constants.ts      # App constants\n│   │   └── helpers.ts        # Helper functions\n│   └── App.tsx               # Main app component\n├── app.json                   # Expo configuration\n└── package.json              # Dependencies\n```\n\n### Data Flow\n\n```\nUser Action (e.g., Generate Image)\n   ↓\n[Screen Component]\n   ↓\n[API Client]\n   ├─ Check Network\n   ├─ Build Request\n   ├─ Add Retry Logic\n   └─ Send to Backend\n   ↓\n[Response]\n   ├─ Success → Cache Result\n   ├─ Error → Retry or Show Error\n   └─ Offline → Use Cached Data\n   ↓\n[Update UI]\n```\n\n### Caching Strategy\n\n**Multi-level Caching:**\n\n1. **Memory Cache** - Fast access, limited size (50 items)\n2. **Disk Cache** - Persistent storage (100MB)\n3. **TTL** - Automatic expiration (1 hour default)\n\n**Cache Key Generation:**\n- Image generation: Hash of prompt\n- Audio: Hash of text\n- Embeddings: Hash of text\n\n## Data Models\n\n### Image Generation Request\n\n```typescript\ninterface ImageGenerationRequest {\n  prompt: string;              // Required: Text description\n  model?: string;              // Optional: Model name\n  negative_prompt?: string;    // Optional: What to avoid\n  height?: number;             // Optional: Image height (default 512)\n  width?: number;              // Optional: Image width (default 512)\n  num_inference_steps?: number; // Optional: Quality steps (default 50)\n  guidance_scale?: number;     // Optional: Prompt adherence (default 7.5)\n}\n```\n\n### Image Editing Request\n\n```typescript\ninterface ImageEditingRequest {\n  image: string;               // Required: Base64 encoded image\n  prompt: string;              // Required: Editing prompt\n  mask?: string;               // Optional: Base64 encoded mask\n  model?: string;              // Optional: Model name\n  negative_prompt?: string;    // Optional: What to avoid\n  strength?: number;           // Optional: Edit strength (0-1)\n  num_inference_steps?: number; // Optional: Quality steps\n  guidance_scale?: number;     // Optional: Prompt adherence\n}\n```\n\n### LLM Request\n\n```typescript\ninterface LLMRequest {\n  messages: Array<{            // Required: Message history\n    role: 'system' | 'user' | 'assistant';\n    content: string;\n  }>;\n  model?: string;              // Optional: Model name\n  max_tokens?: number;         // Optional: Max response length\n  temperature?: number;        // Optional: Randomness (0-2)\n  top_p?: number;              // Optional: Diversity (0-1)\n  top_k?: number;              // Optional: Top-k sampling\n}\n```\n\n## Performance Considerations\n\n### Backend\n\n1. **Model Loading** - Models cached in memory after first use\n2. **Batch Processing** - Can handle multiple requests concurrently\n3. **Image Optimization** - Large images resized before processing\n4. **Connection Pooling** - HTTP connections reused\n\n### Mobile\n\n1. **Image Caching** - Generated images cached locally\n2. **Lazy Loading** - Images loaded only when visible\n3. **Compression** - Images compressed for display\n4. **Async Operations** - Heavy operations in background\n\n## Security Architecture\n\n### API Security\n\n1. **CORS** - Restricted to allowed origins\n2. **HTTPS** - Encrypted in-transit (production)\n3. **Input Validation** - Pydantic models validate all input\n4. **Error Messages** - Generic error messages in production\n\n### API Key Management\n\n1. **HuggingFace Key** - Stored in environment variables\n2. **Never Exposed** - Key never sent to client\n3. **Rotation** - Keys can be rotated without code changes\n\n### Data Privacy\n\n1. **No Data Storage** - API is stateless\n2. **Ephemeral Processing** - Results not persisted\n3. **Client-side Cache** - User controls cached data\n\n## Scalability\n\n### Horizontal Scaling\n\n```\n┌─────────────────────────────────────────┐\n│           Load Balancer                 │\n│  (nginx, AWS ALB, Render Load Balancer) │\n└──────────┬──────────────┬───────────────┘\n           │              │\n      ┌────▼──┐      ┌────▼──┐\n      │Backend│      │Backend│\n      │Instance 1    │Instance 2\n      └────────┘      └────────┘\n```\n\n### Vertical Scaling\n\n- Increase machine RAM (for model loading)\n- Increase CPU cores (for concurrent requests)\n- Use GPU acceleration (for faster inference)\n\n## Monitoring and Observability\n\n### Metrics\n\n1. **Request Metrics**\n   - Request count\n   - Response time\n   - Error rate\n\n2. **Model Metrics**\n   - Inference time\n   - Cache hit rate\n   - Model load time\n\n3. **System Metrics**\n   - CPU usage\n   - Memory usage\n   - Disk usage\n\n### Logging\n\n- Structured JSON logging\n- Log levels: DEBUG, INFO, WARNING, ERROR\n- Request/response logging\n- Error stack traces\n\n## Future Enhancements\n\n1. **Database Integration** - Store user history and preferences\n2. **Authentication** - User accounts and API keys\n3. **Rate Limiting** - Prevent abuse\n4. **Webhooks** - Async processing notifications\n5. **WebSocket Support** - Real-time streaming responses\n6. **Model Fine-tuning** - Custom model training\n7. **Batch Processing** - Process multiple requests together\n8. **Caching Layer** - Redis for distributed caching\n
